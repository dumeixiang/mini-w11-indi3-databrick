## Mini-w11-databrick and  Individual Project #3: Databricks ETL (Extract Transform Load) Pipeline

## Requirements:

Create a data pipeline using Databricks.
Include at least one data source and one data sink.
A well-documented Databricks notebook that performs ETL (Extract, Transform, Load) operations, checked into the repository.
Usage of Delta Lake for data storage (show you understand the benefits).
Usage of Spark SQL for data transformations.
Proper error handling and data validation.
Visualization of the transformed data.
An automated trigger to initiate the pipeline.

## Deliverables:

1. Upload  LA Crime data to databrick.
2. Use Databricks notebook or script to retrieve data
3. Run thorugh ETA pipeline
4. Use notebook to explore and create data visualization.
5. A video demo.


## References
Extract crime data using spark:

![load data](https://github.com/dumeixiang/mini-w11-databrick/blob/main/Screen%20Shot%202023-11-12%20at%208.59.38%20PM.png)

Explore data by data visualization:
![data visualization](https://github.com/dumeixiang/mini-w11-databrick/blob/main/Screen%20Shot%202023-11-12%20at%209.09.14%20PM.png)

Transform data in python enviroment using spark:
![transform data](https://github.com/dumeixiang/mini-w11-databrick/blob/main/Screen%20Shot%202023-11-15%20at%208.46.11%20PM.png)

Insert data uisng delta table:
![inser data](https://github.com/dumeixiang/mini-w11-databrick/blob/main/Screen%20Shot%202023-11-15%20at%208.46.35%20PM.png)

Explore data after tranformation:
![data visualization2](https://github.com/dumeixiang/mini-w11-indi3-databrick/blob/main/Screen%20Shot%202023-11-15%20at%209.08.33%20PM.png)

